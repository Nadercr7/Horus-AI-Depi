# -*- coding: utf-8 -*-
"""Firstmodel_50%.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROa_i6HrzX3SHmYM6TLWqh85cBAODFRh
"""

!pip install -U gdown

import gdown

file_id = "1DkZyM5--du7ToSeyZfPkxIezfAjg6iiw"
gdown.download(f"https://drive.google.com/uc?id={file_id}", output="cleaned_data.zip", quiet=False)

import zipfile

with zipfile.ZipFile("cleaned_data.zip", 'r') as zip_ref:
    zip_ref.extractall("unzipped_data")

import os

# List folders inside unzipped_data
for folder in os.listdir("unzipped_data"):
    print(folder)

import os
print(os.listdir("unzipped_data/cleaned_data"))

import os
import random
import shutil
from PIL import Image
from torchvision import transforms
from tqdm import tqdm

dataset_path = "unzipped_data/cleaned_data"  Ÿç
target_size = (224, 224)
min_resolution = (100, 100) Ÿç
min_images = 100
max_images = 150
min_threshold = 10

augmentation = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomResizedCrop(target_size[0], scale=(0.8, 1.0))
])


def augment_image(image_path, save_path, index):
    image = Image.open(image_path).convert("RGB")
    augmented = augmentation(image)
    augmented = augmented.resize(target_size)
    augmented.save(f"{save_path}/aug_{index}.jpg")
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if not os.path.isdir(class_path):
        continue

    valid_images = []
    for img_name in os.listdir(class_path):
        img_path = os.path.join(class_path, img_name)
        try:
            img = Image.open(img_path).convert("RGB")
            w, h = img.size
            if w < min_resolution[0] or h < min_resolution[1]:
                os.remove(img_path)
                continue
            img = img.resize(target_size)
            img.save(img_path)
            valid_images.append(img_name)
        except:
            if os.path.exists(img_path):
                os.remove(img_path)

    image_count = len(valid_images)

    if image_count < min_threshold:
        shutil.rmtree(class_path)
        print(f"üóëÔ∏è Removed class '{class_name}' (less than {min_threshold} images)")
        continue

    elif image_count > max_images:
        images_to_remove = image_count - max_images
        images_to_delete = random.sample(valid_images, images_to_remove)
        for img in images_to_delete:
            os.remove(os.path.join(class_path, img))
        print(f"üìâ Reduced '{class_name}' to {max_images} images")

    elif image_count < min_images:
        images_needed = min_images - image_count
        valid_images = [img for img in os.listdir(class_path) if img.endswith(('.jpg', '.jpeg', '.png'))]
        if not valid_images:
            print(f"‚ö†Ô∏è Skipping augmentation for '{class_name}' (no valid images found)")
            continue

        for i in range(images_needed):
            img_to_augment = random.choice(valid_images)
            augment_image(os.path.join(class_path, img_to_augment), class_path, i)
        print(f"üÜô Augmented '{class_name}' to {min_images} images")

print("\n‚úÖ ŸÉŸÑ ÿßŸÑŸÉŸÑÿßÿ≥ÿßÿ™ ÿ™ŸÖ ÿ™ŸÜÿ∏ŸäŸÅŸáÿß Ÿàÿ™Ÿàÿ≠ŸäÿØ ÿßŸÑÿµŸàÿ± Ÿàÿ™Ÿàÿßÿ≤ŸÜ ÿπÿØÿØ ÿßŸÑÿµŸàÿ± ÿ®ŸäŸÜŸáŸÖ.")

new_data = {}
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        image_files = [os.path.join(class_path, img) for img in os.listdir(class_path) if img.endswith(('.jpg', '.jpeg', '.png'))]
        if image_files:  # ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑŸÉŸÑÿßÿ≥ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿµŸàÿ±
            new_data[class_name] = image_files
print("\n‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿØÿßÿ™ÿß ÿßŸÑÿ¨ÿØŸäÿØÿ© ŸÅŸä ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± new_data")

# ÿπÿ±ÿ∂ ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑŸÖÿ¨ŸÑÿØÿßÿ™ ŸàÿπÿØÿØ ÿßŸÑÿµŸàÿ± ŸÖŸÜ ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± new_data
for class_name, image_files in new_data.items():
    print(f"Class: {class_name} - Number of images: {len(image_files)}")

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import Sequence

image_paths = []
labels = []
class_names = list(new_data.keys())
class_to_idx = {cls: i for i, cls in enumerate(class_names)}

for class_name, paths in new_data.items():
    for path in paths:
        image_paths.append(path)
        labels.append(class_to_idx[class_name])

train_paths, test_paths, train_labels, test_labels = train_test_split(
    image_paths, labels, test_size=0.2, stratify=labels, random_state=42
)
class CustomDataGenerator(Sequence):
    def __init__(self, image_paths, labels, batch_size, target_size=(224, 224), num_classes=10, shuffle=True):
        self.image_paths = image_paths
        self.labels = labels
        self.batch_size = batch_size
        self.target_size = target_size
        self.num_classes = num_classes
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.image_paths))
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.image_paths) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_paths = [self.image_paths[i] for i in batch_indexes]
        batch_labels = [self.labels[i] for i in batch_indexes]

        X, y = self.__data_generation(batch_paths, batch_labels)
        return X, y

    def __data_generation(self, batch_paths, batch_labels):
        X = np.zeros((self.batch_size, *self.target_size, 3))
        y = np.zeros((self.batch_size, self.num_classes))

        for i, path in enumerate(batch_paths):
            img = load_img(path, target_size=self.target_size)
            img = img_to_array(img) / 255.0
            X[i] = img
            y[i] = to_categorical(batch_labels[i], num_classes=self.num_classes)
        return X, y

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)
train_gen = CustomDataGenerator(train_paths, train_labels, batch_size=32, num_classes=len(class_names))
test_gen = CustomDataGenerator(test_paths, test_labels, batch_size=32, num_classes=len(class_names), shuffle=False)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(len(class_names), activation='softmax')
])

model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.fit(train_gen, epochs=10, validation_data=test_gen)

loss, acc = model.evaluate(test_gen)
print(f"\n‚úÖ ÿØŸÇÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿπŸÑŸâ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±: {acc * 100:.2f}%")

model.save('first_model.h5')

"""# **error analysis**"""

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_probs = model.predict(test_gen)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = test_labels[:len(y_pred)]

print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

import matplotlib.pyplot as plt

# Assuming you have access to test_paths
for i in range(len(y_pred)):
    if y_pred[i] != y_true[i]:
        img = Image.open(test_paths[i])
        plt.imshow(img)
        plt.title(f"True: {class_names[y_true[i]]}, Pred: {class_names[y_pred[i]]}")
        plt.axis('off')
        plt.show()
        break

for i in range(len(y_pred)):
    if y_pred[i] != y_true[i]:

      print(f"True: {class_names[y_true[i]]}, Pred: {class_names[y_pred[i]]}")

import pandas as pd

misclassified_data = []

for i in range(len(y_pred)):
    if y_pred[i] != y_true[i]:
        misclassified_data.append({
            "Image Path": test_paths[i],
            "True Label": class_names[y_true[i]],
            "Predicted Label": class_names[y_pred[i]]
        })

misclassified_df = pd.DataFrame(misclassified_data)

print(misclassified_df.head(10))

misclassified_df["True Label"].value_counts().head(20)

misclassified_df["Predicted Label"].value_counts().head(10)

misclassified_df.sample(30)

misclassified_df[(misclassified_df["True Label"]== "Great Pyramids of Giza")|(misclassified_df["True Label"]=="Khafre Pyramid")]

misclassified_df[["True Label" , "Predicted Label"]].sample(20)

same_classes = {
    "Ramessum" :	"Ramesseum",
    "Mosque of Al Mahmudiyah" : "Mosque_of_al-Mahmudiya",
    "Pyramid of Djoser" : "Pyramid_of_Djoser"	,
    "Colossoi of Memnon" : "Colossi_of_memnon",
    "Pyramid_of_Khafra"	: "Khafre Pyramid",
     "Bibliotheca Alexandrina" : "Bibliotheca_Alexandrina",
     "Great_Hypostyle_Hall_of_Karnak" : "Great Hypostyle Hall of Karnak",
     "Bent pyramid for senefru" : "Bent_Pyramid",
    "sphinx" : "Great_Sphinx_of_Giza",
    "Isis with her child" : "Goddess Isis with her child",
    "Egyptian Museum Tahrir" : "Egyptian_Museum_(Cairo)",


}
classesToDrop = ["Mohandessin","siwa","Great_Pyramid_of_Giza","Pyramid_of_Khafra",
                 "Hatshepsut","Elephantine","Statue of Ramesses II","Giza_pyramid_complex",S"Giza_Plateau",
                 "Garden_City,_Cairo","Gezira","KV62","Mokattam","Mortuary_Temple_of_Hatshepsut",
                 "6_October_Bridge","Nabq_Protected_Area","Pyramid_of_Menkaure"]

import matplotlib.pyplot as plt

def display_images_of_class(image_paths, labels, class_name, class_to_idx, num_images=):
    class_index = class_to_idx[class_name]
    count = 0

    for path, label in zip(image_paths, labels):
        if label == class_index:
            img = load_img(path, target_size=(224, 224))
            img_array = img_to_array(img) / 255.0

            plt.imshow(img_array)
            plt.title(f"Class: {class_name}")
            plt.axis('off')
            plt.show()

            count += 1
            if count >= num_images:
                break

# Example usage
display_images_of_class(image_paths, labels, 'Muizz_Street', class_to_idx)

import os

dataset_path = "unzipped_data/cleaned_data"

classes = os.listdir(dataset_path)

for class_name in classes:
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        num_images = len(os.listdir(class_path))
        print(f"Class: {class_name}, Number of images: {num_images}")

"""# **start to create the second model after error analysis**
 **(in different notebook)**
"""

